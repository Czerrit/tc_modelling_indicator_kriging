{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea0defc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some imports to use in notebok\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "import scipy.spatial.distance as dist\n",
    "from scipy.optimize import curve_fit\n",
    "import matplotlib.patches as mpatches\n",
    "import copy\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from skimage import color\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d9ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a three facies approach:\n",
    "### Using alternative dataset\n",
    "df = pd.read_csv('C:\\\\Users\\\\Gerrit\\\\Desktop\\\\Masterarbeit\\\\Python\\\\changer\\\\Q565756_raw.csv',)\n",
    "data = df.values\n",
    "data = data[0::4]\n",
    "data[:,0] = data[:,0]-data[:,0].min() # I think we should normalize the y-coordinates to be distances (in m)\n",
    "y_max = data[:,0].max()\n",
    "z_max = np.abs(data[:,1].min())# hier Minimum nehmen weil Tiefen negativ sind und dann Absolutwert\n",
    "\n",
    "data_3 = copy.deepcopy(data)\n",
    "\n",
    "\n",
    "data_k1 = copy.deepcopy(data_3)\n",
    "data_k2 = copy.deepcopy(data_3)\n",
    "data_k3 = copy.deepcopy(data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1ae57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=data_3[:,0], y=data_3[:,1], c=data_3[:,2], cmap='viridis')\n",
    "#plt.imshow(im_sub, cmap='gray', origin='lower')\n",
    "plt.title('Bohrlochdaten')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac54913",
   "metadata": {},
   "source": [
    "## 2. Variogram analyis\n",
    "<a id=\"2\"></a>\n",
    "\n",
    "The first step of our geostatistical analysis is to describe the spatial correlation of our variable of interest. We do this in form of a variogram. \n",
    "\n",
    "In order to retrieve the variogram from our data we have to perform multiple steps. For each pointpair in our dataset we can calculate a measure of dissimilarity called *semivariance* ($\\hat{\\gamma}(\\vec{h})$) that is dependent only on the distance between the two locations:\n",
    "\n",
    "$$\\hat{\\gamma}(\\vec{h}) = \\frac{1}{2} \\sum_{\\alpha=1}^n \\left(z(\\vec{x_\\alpha}+\\vec{h}) - z(\\vec{x_\\alpha})\\right)^2 $$\n",
    "\n",
    "Plotting all semivariances against the so called *lag distance* ($\\vec{h}$) leads to a plot called the *variogram cloud*, that is often quite messy due to the number of point pairs. To resolve this issue lag distances are binned based on a lag tolerance ($\\Delta \\vec{h}$) and the corresponding *semivariances* are averaged for each bin to create a plot called the *experimental varigoram*.\n",
    "the customised function looks as follows:\n",
    "$$\\hat{\\gamma}(\\vec{h}) = \\frac{1}{2} \\sum_{\\alpha=1}^n \\left(z(\\vec{x_\\alpha}+\\vec{h}) - z(\\vec{x_\\alpha})\\right)^2*log(1+X)$$ \n",
    "In this function, X is a matrix that contains the distance in x-direction between all points.\n",
    "between them looks like this:\n",
    "The following function performs all the steps to create an *experimental variogram* based on a given dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c06d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_variogram(x_pos, y_pos, vals, bin_size, start=0, stop=180):\n",
    "    \"\"\"Calculate experimental variogram for sampled values\n",
    "    \n",
    "    **Arguments**:\n",
    "        - x_pos (np.array) : x-coordiantes of sampled locations\n",
    "        - y_pos (np.array) : y-coordiantes of sampled locations\n",
    "        - vals (np.array) : values at corresponding x-y positions\n",
    "        - bin_size (float) :bin size\n",
    "        - start (float) : minimum lag distance\n",
    "        - stop (float) : maximum lag distance\n",
    "        \n",
    "    **Returns**:\n",
    "        - h_bins (np.array) = average lag distances for bins\n",
    "        - ave_vals (np.array) = average semivariances for bins\n",
    "    \"\"\"\n",
    "    X = np.vstack([x_pos, y_pos]).transpose()\n",
    "    \n",
    "    h = dist.squareform(dist.pdist(X))\n",
    "    n = len(x_pos)\n",
    "\n",
    "    gamma = np.empty((n, n))\n",
    "    \n",
    "    for pos in range (n):\n",
    "        gamma[pos, :] = ((vals[:]-vals[pos])**2)/2 * np.log(1 + np.abs(X[:, 0] - X[pos, 0]))\n",
    "    \n",
    "    # Second step: Bins and stuff\n",
    "    maxim = stop-start\n",
    "    bins = maxim/bin_size\n",
    "    bins = int(bins)\n",
    "\n",
    "    h_bins = np.empty(bins)\n",
    "    ave_vals = np.empty(bins)\n",
    "\n",
    "    for j in range(bins):\n",
    "        # create mask\n",
    "        mask = np.where((h > start) & (h < (start+bin_size)))\n",
    "        # use mask\n",
    "        ave_vals[j] = np.average(gamma[mask])\n",
    "        h_bins[j] = (start+start+bin_size)/2\n",
    "        start += bin_size\n",
    "        \n",
    "    ## mask if there start to be nans in the ave_vals (ich nehme an das passiert wenn die Distanz größer wird als die Distanz der Daten)\n",
    "    mask = ~np.isnan(ave_vals)\n",
    "\n",
    "    return h_bins[mask], ave_vals[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a466d1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a discrete grid to estimate on (reasonable resolution)\n",
    "x = np.linspace(0, y_max,20)\n",
    "y = np.linspace(-z_max,5,20)\n",
    "locs = np.vstack((x,y))\n",
    "locs = locs.T\n",
    "\n",
    "xv, yv = np.meshgrid(x, y)\n",
    "positions = np.vstack([xv.ravel(), yv.ravel()]).T\n",
    "\n",
    "# just showing grid locations\n",
    "plt.scatter(positions[:,0], positions[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ae8d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining experimental_varogramm to automatical fit the data\n",
    "def exponential_variogram_model(d, range_, sill):\n",
    "    '''Exponential variogram function\n",
    "    \n",
    "    **Arguments**:\n",
    "        - d (np.array) : positions to evaluate function\n",
    "        - range_ (float) : range parameter of model\n",
    "        - sill (np.array) : sill parameter of model\n",
    "        \n",
    "    **Returns**:\n",
    "        - gamma (np.array) : theoretical exponential semivariance\n",
    "    '''\n",
    "    nugget = 0 # Advanced \n",
    "    psill = sill- nugget\n",
    "    gamma = psill * (1. - np.exp(-d / (range_))) + nugget\n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e593f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining three subdatasets based on indicator function\n",
    "\n",
    "# first threshold for facies 1 \n",
    "mask1 = data_k1[:,2]>0\n",
    "data_k1[:,2][mask1]=0\n",
    "data_k1[:,2][~mask1]=1\n",
    "\n",
    "# second threshold for facies 2\n",
    "mask2 = data_k2[:,2]==1\n",
    "data_k2[:,2][mask2]=1\n",
    "data_k2[:,2][~mask2]=0\n",
    "\n",
    "# third threshold for facies 3 \n",
    "mask3 = data_k3[:,2]>1\n",
    "data_k3[:,2][mask3]=1\n",
    "data_k3[:,2][~mask3]=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc56db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we would actually require a step with a variogram for each threshold!\n",
    "\n",
    "h_bins_k1, ave_vals_k1 = full_variogram(data_3[:,0], data_3[:,1], data_k1[:,2], 5)\n",
    "h_bins_k2, ave_vals_k2 = full_variogram(data_3[:,0], data_3[:,1], data_k2[:,2], 5)\n",
    "h_bins_k3, ave_vals_k3 = full_variogram(data_3[:,0], data_3[:,1], data_k3[:,2], 5)\n",
    "\n",
    "# automatied fitting\n",
    "variogram_parameters1, par_b1 = curve_fit(exponential_variogram_model, h_bins_k1, ave_vals_k1)\n",
    "variogram_parameters2, par_b2 = curve_fit(exponential_variogram_model, h_bins_k2, ave_vals_k2)\n",
    "variogram_parameters3, par_b3 = curve_fit(exponential_variogram_model, h_bins_k3, ave_vals_k3)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(1,3, figsize=(16,4))\n",
    "\n",
    "ax[0].plot(h_bins_k1, ave_vals_k1, 'o')\n",
    "ax[1].plot(h_bins_k2, ave_vals_k2, 'o')\n",
    "ax[2].plot(h_bins_k3, ave_vals_k3, 'o')\n",
    "\n",
    "# again here just eyeballing\n",
    "x_temp = np.linspace(0,y_max,100)\n",
    "ax[0].plot(x_temp, exponential_variogram_model(x_temp, variogram_parameters1[0], variogram_parameters1[1]))\n",
    "ax[1].plot(x_temp, exponential_variogram_model(x_temp, variogram_parameters2[0], variogram_parameters2[1]))\n",
    "ax[2].plot(x_temp, exponential_variogram_model(x_temp, variogram_parameters3[0], variogram_parameters3[1]))\n",
    "\n",
    "print(variogram_parameters1)\n",
    "print(variogram_parameters2)\n",
    "print(variogram_parameters3)\n",
    "\n",
    "#Plotting range parameter \n",
    "#ax[0].plot((variogram_parameters1[0], variogram_parameters1[0]), np.linspace(0, variogram_parameters1[1], 2))\n",
    "#ax[1].plot((variogram_parameters2[0], variogram_parameters2[0]), np.linspace(0, variogram_parameters2[1], 2))\n",
    "#ax[2].plot((variogram_parameters3[0], variogram_parameters3[0]), np.linspace(0, variogram_parameters3[1], 2))\n",
    "\n",
    "# labeling\n",
    "ax[0].set_title('Experimentelles Semivariogramm für Sand')\n",
    "ax[0].set_xlabel('Lag Distanz $h$')\n",
    "ax[0].set_ylabel('Semivarianz $\\gamma^*$')\n",
    "ax[1].set_title('Experimentelles Semivariogramm für Schluff')\n",
    "ax[1].set_xlabel('Lag Distanz $h$')\n",
    "ax[1].set_ylabel('Semivarianz $\\gamma^*$')\n",
    "ax[2].set_title('Experimentelles Semivariogramm für Ton')\n",
    "ax[2].set_xlabel('Lag Distanz $h$')\n",
    "ax[2].set_ylabel('Semivarianz $\\gamma^*$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a66d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform kriging for all three thesholds\n",
    "results_k1 = kriging_grid(data_k1, positions, range_=variogram_parameters1[0], sill=variogram_parameters1[1])\n",
    "results_k2 = kriging_grid(data_k2, positions, range_=variogram_parameters2[0], sill=variogram_parameters2[1])\n",
    "results_k3 = kriging_grid(data_k3, positions, range_=variogram_parameters3[0], sill=variogram_parameters3[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b315b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting resultung probability maps\n",
    "fig, ax = plt.subplots(1,3, figsize=(16,6))\n",
    "\n",
    "ax[0].set_ylim(-z_max,5)\n",
    "ax[0].set_xlim(5,y_max)\n",
    "\n",
    "ax[1].set_ylim(-z_max,5)\n",
    "ax[1].set_xlim(5,y_max)\n",
    "\n",
    "ax[2].set_ylim(-z_max,5)\n",
    "ax[2].set_xlim(5,y_max)\n",
    "\n",
    "# creating the contourf kriging result\n",
    "x = np.linspace(0, y_max,20)\n",
    "y = np.linspace(-z_max,5,20)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z1 = results_k1.reshape(X.shape)\n",
    "Z2 = results_k2.reshape(X.shape)\n",
    "Z3 = results_k3.reshape(X.shape)\n",
    "\n",
    "CS1 = ax[0].contourf(X, Y, Z1, 10, cmap=plt.cm.viridis)\n",
    "CS2 = ax[1].contourf(X, Y, Z2, 10, cmap=plt.cm.viridis)\n",
    "CS2 = ax[2].contourf(X, Y, Z3, 10, cmap=plt.cm.viridis)\n",
    "\n",
    "# plotting input data\n",
    "#ax[0].scatter(data[:,0], data[:,1], color='black')\n",
    "#ax[1].scatter(data[:,0], data[:,1], color='black')\n",
    "#ax[2].scatter(data[:,0], data[:,1], color='black')\n",
    "\n",
    "ax[0].set_aspect('equal', adjustable='box')\n",
    "ax[1].set_aspect('equal', adjustable='box')\n",
    "ax[2].set_aspect('equal', adjustable='box')\n",
    "\n",
    "ax[0].title.set_text('Wahrscheinlichkeit Vorhandensein von Sand')\n",
    "ax[1].title.set_text('Wahrscheinlichkeit Vorhandensein von Schluff')\n",
    "ax[2].title.set_text('Wahrscheinlichkeit Vorhandensein von Ton')\n",
    "\n",
    "cb_ax = fig.add_axes([0.95, 0.15, 0.03, 0.7])\n",
    "cbar = fig.colorbar(CS1, cax=cb_ax)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913af57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating structur modell\n",
    "# combining results for all three facies\n",
    "results_comb = np.vstack((results_k1, results_k2, results_k3)).T\n",
    "\n",
    "# finding facies with highest probability for each location\n",
    "highest_prob = np.argmax(results_comb, axis=1)\n",
    "\n",
    "# Manuell festgelegte Farben für Sand, Schluff und Ton\n",
    "custom_colors = ['#FFD700', '#04B404', '#A52A2A']\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "Z4 = highest_prob.reshape(X.shape)\n",
    "\n",
    "# Verwenden Sie eine benutzerdefinierte Colormap mit den festgelegten Farben\n",
    "custom_cmap = ListedColormap(custom_colors, name='custom_cmap', N=len(custom_colors))\n",
    "\n",
    "# Plotting mit den zugeordneten Farben\n",
    "CS4 = ax.imshow(Z4, origin='lower', cmap=custom_cmap, extent=[0, y_max, -z_max, 0], vmin=0, vmax=len(custom_colors)-1)\n",
    "\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "# Ändern Sie die Schriftgröße des Titels\n",
    "ax.title.set_text('Strukturmodell')\n",
    "ax.title.set_fontsize(24)  # Hier können Sie die gewünschte Schriftgröße festlegen\n",
    "\n",
    "# Eindeutige Werte und Farben für die Legende\n",
    "values = [0, 1, 2]\n",
    "\n",
    "# Erstellen Sie Patches für die Legende mit benutzerdefinierten Labels\n",
    "patches = [mpatches.Patch(color=custom_colors[value], label=\" {l}\".format(l=('Sand', 'Schluff', 'Ton')[value])) for value in values]\n",
    "\n",
    "# Legende hinzufügen\n",
    "plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, prop={'size': 20})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccad474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thermal conductivity map based on structure model\n",
    "\n",
    "thermal_conductivity_values = [2.78, 1.83, 1.52]\n",
    "\n",
    "# Transfer to categorical map\n",
    "results_comb = np.vstack((results_k1, results_k2, results_k3)).T\n",
    "highest_prob = np.argmax(results_comb, axis=1)\n",
    "Z4 = highest_prob.reshape(X.shape)\n",
    "\n",
    "# Plotting mit Wärmeleitfähigkeitswerten\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Benutzen Sie den Z-Wert, um die entsprechenden Wärmeleitfähigkeitswerte zu erhalten\n",
    "thermal_conductivity_map = np.array([thermal_conductivity_values[z] for z in Z4.flatten()])\n",
    "\n",
    "# Reshape für die Darstellung\n",
    "thermal_conductivity_map = thermal_conductivity_map.reshape(Z4.shape)\n",
    "\n",
    "# Darstellung des Wärmeleitfähigkeitsmodells\n",
    "CS4 = ax.imshow(thermal_conductivity_map, origin='lower', cmap=plt.cm.plasma, extent=[0, y_max, -z_max, 0])\n",
    "\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.title.set_text('Wärmeleitfähigkeitsmodell')\n",
    "ax.title.set_fontsize(24)  # Hier können Sie die gewünschte Schriftgröße festlegen\n",
    "\n",
    "# Legende für Wärmeleitfähigkeitswerte erstellen und die Größe anpassen\n",
    "cbar = plt.colorbar(CS4, ax=ax, orientation='vertical', shrink=0.8)  # Sie können den shrink-Wert anpassen\n",
    "cbar.set_label('Wärmeleitfähigkeitswert (in W/mK)', rotation=270, labelpad=15)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
